{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3768310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env CHETANBASE_SECRET_KEY=secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chetan import SessionManager, ChetanbaseClient\n",
    "\n",
    "\n",
    "mgr = SessionManager(ChetanbaseClient(\"<api_url>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b84e5c8",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c237d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chetan.lm.local import LMTransformers\n",
    "from chetan.lm.openai import LMOpenAI\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "from chetan.lm.groq import LMGroq\n",
    "from chetan.lm.anthropic import LMAnthropic\n",
    "\n",
    "import os\n",
    "\n",
    "# # Load a local model for maximum control\n",
    "# lm = LMTransformers(\"NousResearch/DeepHermes-3-Llama-3-8B-Preview\")\n",
    "\n",
    "# Supports OpenAI and Azure OpenAI\n",
    "lm = LMOpenAI(\n",
    "    client=AzureOpenAI(\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_KEY\"),\n",
    "        api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    "    ),\n",
    "    model=\"gpt-4.1\" \n",
    "    # model=\"gpt-4.1-nano\", # * Works like a charm, despite being a nano model\n",
    "    \n",
    "    # # ! Local vLLM server, couldn't test due to GPU memory constraints\n",
    "    # client=OpenAI(base_url=\"http://192.168.10.179:8000/v1\", api_key=\"none\"), \n",
    "    # model=\"NousResearch/Hermes-3-Llama-3.2-3B\",\n",
    ")\n",
    "\n",
    "# # Supports Anthropic\n",
    "# lm = LMAnthropic()\n",
    "\n",
    "# # Supports Groq\n",
    "# lm = LMGroq()\n",
    "\n",
    "# Register the language model as default\n",
    "mgr.lm[\"default\"] = lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095d4d5",
   "metadata": {},
   "source": [
    "### Agent Architecture\n",
    "#### Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd222a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from chetan.modules import rag, memory, recommender\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "rag_module = rag.LlamaIndexRAGModule(\n",
    "    lambda: SimpleDirectoryReader(\"/Users/arjo/Downloads/rag_data\").load_data(),\n",
    "    embed_model_fn=lambda: HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\"),\n",
    ")\n",
    "memory_module = memory.SmritiMemoryModule()\n",
    "tool_rec_module = recommender.ToolRecommenderModule(\n",
    "    embedder=\"NovaSearch/stella_en_400M_v5\"\n",
    ")\n",
    "\n",
    "### DONE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335ea78",
   "metadata": {},
   "source": [
    "### Agent Loop\n",
    "\n",
    "These are the stages of an agent loop:\n",
    "- Prologue (every function run in parallel)\n",
    "- Process (Agent text generation with tool calls)\n",
    "- Epilogue (every function run in paralle)\n",
    "- Retrigger (a condition that must be fulfilled to reiterate the loop, generally user approval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chetan.agent import AgentLoop\n",
    "from chetan.agent.loop import ProcessFunctionContext\n",
    "\n",
    "agentloop = AgentLoop(mgr).use(\n",
    "    # rag_module,\n",
    "    memory_module,\n",
    "    tool_rec_module,\n",
    ")\n",
    "\n",
    "\n",
    "@agentloop.process\n",
    "async def process_fn(ctx: ProcessFunctionContext):\n",
    "    await ctx.generate_with_tool_call()\n",
    "    await ctx.execute_tool_calls()\n",
    "\n",
    "\n",
    "mgr.agentloop[\"default\"] = agentloop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786043f",
   "metadata": {},
   "source": [
    "### Tools\n",
    "#### Local tool\n",
    "##### Code Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chetan.tools import Tool, toolfn\n",
    "from agentrun import AgentRun\n",
    "\n",
    "\n",
    "class CodeExecutor(Tool):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.runner: AgentRun = AgentRun(container_name=\"agentrun-api-api-1\")\n",
    "\n",
    "    @toolfn\n",
    "    def run_code(self, code: str) -> str:\n",
    "        \"\"\"\n",
    "        Run the given code in the container.\n",
    "        \"\"\"\n",
    "        # Use the agentrun container to run the code\n",
    "        result = self.runner.execute_code_in_container(code)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f7dbf",
   "metadata": {},
   "source": [
    "##### Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a85854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "class WebCrawler(Tool):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.crawler = AsyncWebCrawler()\n",
    "\n",
    "    @toolfn\n",
    "    async def crawl(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Crawl the given URL and return the content.\n",
    "        \"\"\"\n",
    "        return (await self.crawler.arun(url)).markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae754d9",
   "metadata": {},
   "source": [
    "#### MCP\n",
    "Supports both local and remote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chetan.tools import ToolNamespace\n",
    "from chetan.tools.mcp import MCPLoader\n",
    "from mcp import StdioServerParameters\n",
    "\n",
    "mcp_tool_paths = {\n",
    "    # ! US National Weather Service API\n",
    "    \"weather\": \"/Users/arjo/Work/self/mcp_test/weather.py\",\n",
    "    # \"http://192.168.10.225:8000/sse\", # * Same as the one above, but using a HTTP SSE endpoint\n",
    "    # These two on the top are the same\n",
    "    # ! Tavily Search\n",
    "    \"tavily\": StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[\"-m\", \"mcp_server_tavily\"],\n",
    "        env={\"TAVILY_API_KEY\": os.getenv(\"TAVILY_API_KEY\")},\n",
    "    ),\n",
    "    # # ! GitHub API\n",
    "    # \"github\": StdioServerParameters(\n",
    "    #     command=\"docker\",\n",
    "    #     args=[\n",
    "    #         \"run\",\n",
    "    #         \"-i\",\n",
    "    #         \"--rm\",\n",
    "    #         \"-e\",\n",
    "    #         \"GITHUB_PERSONAL_ACCESS_TOKEN\",\n",
    "    #         \"ghcr.io/github/github-mcp-server\",\n",
    "    #     ],\n",
    "    #     env={\"GITHUB_PERSONAL_ACCESS_TOKEN\": os.getenv(\"GITHUB_PERSONAL_ACCESS_TOKEN\")},\n",
    "    # ),\n",
    "    # # ! Notion API\n",
    "    # \"notion\": StdioServerParameters(\n",
    "    #     command=\"npx\",\n",
    "    #     args=[\"-y\", \"@notionhq/notion-mcp-server\"],\n",
    "    #     env={\n",
    "    #         \"OPENAPI_MCP_HEADERS\": '{\"Authorization\": \"Bearer '\n",
    "    #         + os.getenv(\"NOTION_INTEGRATION_SECRET\")\n",
    "    #         + '\", \"Notion-Version\": \"2022-06-28\" }'\n",
    "    #     },\n",
    "    # ),\n",
    "}\n",
    "\n",
    "mcp_tools = await MCPLoader.load_from_paths(\n",
    "    mcp_tool_paths,\n",
    "    # python_default=\"/Users/arjo/Work/self/mcp_test/.venv/bin/python\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62a288",
   "metadata": {},
   "source": [
    "##### Adding tools to the registry namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a33693",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, tool in mcp_tools.items():\n",
    "    mgr.tools.register(\"mcp.\" + name, tool)\n",
    "    \n",
    "# mgr.tools.register(\"code\", CodeExecutor())\n",
    "mgr.tools.register(\"web\", WebCrawler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.tools.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chetan.agent import Agent\n",
    "from chetan.entity.user import User\n",
    "\n",
    "mgr.agents.add(\n",
    "    Agent(\n",
    "        id=\"support_agent\",\n",
    "        role=\"Support Agent\",\n",
    "        description=\"A customer support agent with supervision\",\n",
    "        # system_prompt=\"You are a support agent. Provide assistance to the user thereby. If clarification is needed, ask the supervisor.\",\n",
    "        system_prompt=\"\"\"You are a helpful agent operating in a loop of observation and action.\n",
    "\n",
    "You cannot use any tool without first informing the user of your intention. Only after writing to the user do you automatically invoke the tool in the same response output.\n",
    "\n",
    "Always use <|stop|> when you have fully answered the user's query and do not need to call any tools or wait for more information. \n",
    "Do NOT use <|stop|> if you are about to call a tool or need to continue the loop.\n",
    "\n",
    "After you have provided a direct answer to the user's query, always output <|stop|> unless the user has explicitly asked for further details, follow-up, or another action.\n",
    "\n",
    "General guidelines for <|stop|>:\n",
    "- Use it to end the loop when you have provided a complete answer to the user's query.\n",
    "- Always use it to wait and ask for more information from the user when needed.\n",
    "\"\"\",\n",
    "    )\n",
    ")\n",
    "    \n",
    "\n",
    "mgr.users.add(\n",
    "    User(id=\"customer\", description=\"A customer seeking support\"),\n",
    "    User(id=\"supervisor\", description=\"A supervisor overseeing support\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.agents[\"support_agent\"].context.clear(retain_system_prompt=True)\n",
    "await mgr.agents[\"support_agent\"].prompt(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr.agents[\"support_agent\"].context.save_json(\"./ctx.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Trial if agent is exiting the loop without endless loop\n",
    "for _ in range(3):\n",
    "    mgr.agents[\"support_agent\"].context.clear(retain_system_prompt=True)\n",
    "    await mgr.agents[\"support_agent\"].prompt(\"How are you doing?\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727280f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class FinancialReport(BaseModel):\n",
    "    revenue: float = 0.0\n",
    "    net_income: float = 0.0\n",
    "    earnings_per_share: float = 0.0\n",
    "    total_assets: float = 0.0\n",
    "    total_liabilities: float = 0.0\n",
    "    total_equity: float = 0.0\n",
    "    operating_income: float = 0.0\n",
    "    gross_profit: float = 0.0\n",
    "    tax_expense: float = 0.0\n",
    "    cash_flow_operations: float = 0.0\n",
    "    cash_flow_investing: float = 0.0\n",
    "    cash_flow_financing: float = 0.0\n",
    "    fiscal_year: str = \"\"\n",
    "    quarter: int = 0\n",
    "    currency: str = \"USD\"\n",
    "    company_name: str = \"\"\n",
    "    debt_to_equity_ratio: float = 0.0\n",
    "    current_ratio: float = 0.0\n",
    "    return_on_assets: float = 0.0\n",
    "    return_on_equity: float = 0.0\n",
    "    profit_margin: float = 0.0\n",
    "    segment_revenues: dict = {}\n",
    "    tax_rate: float = 0.0\n",
    "    dividend_payout: float = 0.0\n",
    "    research_development_expense: float = 0.0\n",
    "\n",
    "await mgr.agents[\"support_agent\"].prompt(\n",
    "    \"Provide a report on Google finance statement regarding their income, sales, taxes in the last 2 years\",\n",
    "    FinancialReport\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233000ca",
   "metadata": {},
   "source": [
    "#### Manifesto\n",
    "\n",
    "A rule can check on these:\n",
    "- Agent Output (from LLM)\n",
    "- Structured outputs (from LLM)\n",
    "- Explicit communication messages (in a System)\n",
    "- Structured protocols (in a System)\n",
    "- Tool Calls\n",
    "\n",
    "It can possible also check:\n",
    "- User behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b47a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chetan.manifesto import Manifesto, validator_on_message, ValidatorFeedback, Connection\n",
    "from chetan.manifesto.rules import Rule\n",
    "from chetan.types.context.agent import EntityMessage\n",
    "\n",
    "\n",
    "class CustomerSupportSystemManifesto(Manifesto):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # These will be injected into the agent's system prompt\n",
    "        self.purpose = \"The customer support system is designed to assist users with their queries while maintaining a professional and respectful communication environment.\"\n",
    "        self.rules.extend(\n",
    "            {\n",
    "                \"no_cuss_word\": Rule(\n",
    "                    \"Never use any cusswords or offensive language in any communication with the user.\",\n",
    "                    connections=[\n",
    "                        Connection(\"support_agent\", \"*\"),  # support agent to anyone\n",
    "                    ],\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        super().__init__(\"Customer Support System Manifesto\")\n",
    "\n",
    "    @validator_on_message(\"no_cuss_word\")\n",
    "    def validate_cuss_word(self, message: EntityMessage) -> bool:\n",
    "        message_text = message.content.lower()\n",
    "        cusswords = []  # some cusswords to check against\n",
    "        for cussword in cusswords:\n",
    "            if cussword in message_text:\n",
    "                return ValidatorFeedback(\n",
    "                    score=-2, message=\"Use of cussword is strictly prohibited\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74adcb6d",
   "metadata": {},
   "source": [
    "### System\n",
    "```mermaid\n",
    "graph LR\n",
    "    customer <--> agent <--> supervisor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chetan.system.sequential import SequentialSystem\n",
    "\n",
    "mgr.system = SequentialSystem(mgr).create(\"customer\", \"support_agent\", \"supervisor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chetan.orchestra.chat import ChatOrchestrator\n",
    "\n",
    "chat = ChatOrchestrator(mgr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mgr.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chat.user(\"customer\").handle_incomeing_messages\n",
    "def handle_incoming_messages(self, message):\n",
    "    # Process the incoming message from the customer\n",
    "    print(f\"Customer message received: {message.content}\")\n",
    "    # Here you can add logic to handle the message, e.g., logging, processing, etc.\n",
    "    return \"Message received and being processed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.user(\"customer\").to(\"support_agent\").message(\"Can you summarize your privacy policy?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
